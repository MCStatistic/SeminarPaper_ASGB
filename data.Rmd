---
title: "Data Wrangling"
author: "Marlon Schumacher"
date: "19 11 2018"
output: html_document
---

## Source of Data

[DIP (Dokumentations- und Informationssystem für Parlamentarische Vorgänge)](https://dipbt.bundestag.de/dip21.web/bt)

## Creating the URL's & file-paths for 19. BTD

Last document of the 19. Bundestag
http://dipbt.bundestag.de/dip21/btd/19/000/1900001.pdf

First Document of the 19. Bundestag (Update is comming)
http://dipbt.bundestag.de/dip21/btd/19/057/1905799.pdf

```{r}
# just use pacman, it's delicous!
library(pacman)
p_load(magrittr, dplyr, stringr, downloader, purrr, pdftools, 
       glue, tm, janeaustenr, tidytext, corpus)

# short code for creating all 5800 URL's (19. BTD)
# rep() replicates the object n-times
base_19 <- rep("http://dipbt.bundestag.de/dip21/btd/19/0", each = 5800) 

# code explaining: replicate the character ".pdf" 5800 times
end_19 <- rep(".pdf", each = 5800)
mid_19 <- rep("/190", each = 5800)

# first step: creating a vector with the values from 00 up to 57 as characters
# second step: creating a vector where each value is replicated 100 times
# code exp: create a vector with the values of 00 to 57, all numbers
#           should be 2 digit long (defined by "%02.0f") and should be
#           defined as characters. Then take this vector and replicate it 100 times
first_19 <- as.character(sprintf("%02d", 00:57)) %>% 
  rep(each = 100) 

# creating a vector which includes the values 0000 up to 5799
# important: sprintf() is used for the 4-digit format
second_19 <- as.character(sprintf("%04d", 0000:5799))

# code exp: take all the vectors and combine these vectors with the function str_c()
#           sep = "" -> there is no seperation, otherwise it would be not a functonal url
urls_19 <- str_c(base_19, first_19, mid_19, second_19, end_19, 
              sep = "")

# checking url
head(urls_19, 2)
tail(urls_19, 2)

# checking length
length(urls_19)

# deleting the first link because theres no content
urls_19 <- urls_19[!urls_19 == "http://dipbt.bundestag.de/dip21/btd/19/000/1900000.pdf"]

# checking length again
length(urls_19)

# creating file-path for each link
path_19 <- paste("./19_btd/", basename(urls_19), sep = "")
length(path_19)
```

## Creating URL's for the 18. BTD

Last document of the 18. Bundestag
http://dipbt.bundestag.de/dip21/btd/18/137/1813705.pdf

First Document of the 18. Bundestag
http://dipbt.bundestag.de/dip21/btd/18/000/1800001.pdf

Documents 13700 up to 13705 doesn't include any type of document which is of interest

```{r}
# static body
# IMPRTANT: check/validate the links for the 18 BDT!
base_18 <- rep("http://dipbt.bundestag.de/dip21/btd/18/", each = 13700) 
end_18 <- rep(".pdf", each = 13700)
mid_18 <- rep("/18", each = 13700)

# dynamic body
first_18 <- as.character(sprintf("%03d", 000:136)) %>% 
  rep(each = 100)

second_18 <- as.character(sprintf("%05d", 00000:13699))

# combine all url-elements
urls_18 <- str_c(base_18, first_18, mid_18, second_18, end_18, spe = "")

# checking the URLs
head(urls_18, 2)
tail(urls_18, 2)

# checking length
length(urls_18)

# deleting first URL because there is no content
urls_18 <- urls_18[!urls_18 == "http://dipbt.bundestag.de/dip21/btd/18/000/1800000.pdf"]

# checking length again
length(urls_18)

# creating file-path for each link
path_18 <- paste("./18_btd/", basename(urls_18), sep = "")
length(path_18)
```


## Testing
```{r}
# testing for loop with url_printing
for(i in seq_along(urls_test)){
    download(urls_test[i], destination_test[i], mode="wb")
  print(urls_test[i])
}

# using map2() instead of a for loop
```

## live coding with fabio <3 & Download 18. BTD
```{r}
# fn + F2 :)
download

# creating sleep function
# take the function download() and set the sleep to 1.5 seconds
sleep_down <- function(...) {
  download(...)
  Sys.sleep(0.5)
}

# creating safely
# code exp: take the function sleep_down() and create with the function safely() a new function
#           because of the safely function, the loop will not interrupt, if it comes to errors
safe_download <- safely(sleep_down)

# downloading the documents for the 18. Bundestag
# code exp: map2() can use two arguments -> take the vector urls as the first argument 
#           and the vector destinations as the second argument
#           
map_results_18 <- map2(urls_18, path_18,
                    ~safe_download(.x, .y, mode ="wb"))



# converting list into two vectors as.character()
results_18 <- map(map_results_18, "result") %>% 
  as.character() 
error_18 <- map(map_results_18, "error") %>% 
  as.character()

# creating df
tibble(results_18, error_18)
```


## Download 19. BTD
```{r}
# downloading all documents for the 19. Bundestag (cut off: end of Nov 2018)
map_results_19 <- map2(urls_19, path_19,
                       ~safe_download(.x, .y, mode = "wb"))

# creating two vectors regarding record of errors again
results_19 <- map(map_results_19, "results") %>% 
  as.character()
error_19 <- map(map_results_19, "error") %>% 
  as.character()

# creating df again
tibble(results_19, error_19)
```


## Data Wrangling with PDF Documents

[tidyless introduction to it](https://www.r-bloggers.com/how-to-extract-data-from-a-pdf-file-with-r/)

Used package: [pdftools](https://cran.r-project.org/web/packages/pdftools/pdftools.pdf)

[very great tutorial](https://www.brodrigues.co/blog/2018-06-10-scraping_pdfs/)


```{r}
# setting options for text mining
options(stringsAsFactors = FALSE)

file_1 <- pdf_text("./18_btd/1800038.pdf") %>% 
  readr::read_lines() %>% 
  unlist()

file_test <- map()

full_file <- path_18[1:10] %>% 
  map(~pdf_text(.x))

readr::read_lines(full_file[[1]])
full_file[1] %>% 
  unlist() %>% 
  str_split("\n") %>% 
  unlist()

pdf_text(path_18[1])

corpus <- Corpus(VectorSource(full_file))
readr::read_lines(corpus$content[1])

class(file_2)


file_2 <- pdf_text("./18_btd/1800038.pdf") %>% 
  unlist()

file_corpus <- Corpus(VectorSource(file))
file_corpus$content

# removing stop words
head(stopwords("de"))
tm_map(corpus, removeWords, stopwords("de"))

file <- str_split(file, "\n")

# cat() makes it more readable
cat(file[1])

testing <- pdf_text("./18_btd/1800038.pdf") %>% 
  strsplit("\n") %>% 
  unlist()

pdf_text_2 <- function(pdf){
  testing <- pdf_text(pdf) %>% 
    strsplit("\n") %>% 
    unlist()
}

pdf_vector_test <- c("./18_btd/1800038.pdf", "./18_btd/1800039.pdf", "./18_btd/1800040.pdf")
map_test <- map(pdf_vector_test,
                ~pdf_text_2(.x))

```

suport vector model Machine Learning



# corpus dtype

This procedure is not used for data manipulation! However, the code exists for documentation.
```{r}
# using tm package for reading pdf-documents
read <- readPDF(engine = "xpdf", 
                control = list(text = "-layout"))

document <- Corpus(URISource("./18_btd/1800038.pdf"), 
                   readerControl = list(reader = read))

# creating test_vector for pdf reading
pdf_vector_test <- c("./18_btd/1800038.pdf", "./18_btd/1800039.pdf", "./18_btd/1800040.pdf")

document2 <- Corpus(URISource(pdf_vector_test, encoding = "UTF-8"), 
                   readerControl = list(reader = read))

content(document2[[3]])

meta(document2[[3]])

doc_2 <- content(document2[[1]])
head(doc_2, 15)

doc_2 %>% 
  str_replace_all("\xdc", "ü") %>% 
  str_replace_all("\xfc", "ü") %>% 
  str_replace_all("\xf6", "ö") 
```

