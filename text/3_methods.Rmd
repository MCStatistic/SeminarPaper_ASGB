---
output: pdf_document
---
```{r chunks, include=FALSE}
# Default Options - kann bei Gebrauch geändert werden
knitr::opts_chunk$set(
  echo = T # Whether to display code along with its results
  , eval = T # Whether to evaluate the code and include its results
  , results = "asis" # this at deafult is in end much more efficient
  , cache = F # Whether to cache results for future renders (efficient!)
  , warning = F # Whether to display errors
  , message = F # Whether to display messages
  , error = F # maybe turn on
  , tidy = F # Whether to reformat code in a tidy way when displaying it
  , fig.width = 6 # plot width at 6
  , fig.height = 4 # plot height at 4
  , fig.align = "left" # plot alignment center
)

options(xtable.comment = FALSE, scipen = 9999)

#devtools::install_github("ropensci/rcrossref")
# cat(rcrossref::cr_cn(dois = "10.1126/science.169.3946.635", format = "bibtex"))
# remedy::set_hotkeys()

pacman::p_load(tidyverse, gridExtra)

```


Im Folgenden soll die methodische Vorgehensweise erläutert werden, welche die Datenbeschaffung, die Klassifizierung sowie die anschließende Analyse beschreiben soll. Da es keinen Datensatz gibt, welcher sämtliche kleine Anfragen sowie deren thematische Klassifizierung enthält, mussten die Daten eigenständig erhoben und klassifiziert werden. Bei den kleinen Anfragen handelt es sich um Drucksachen des deutschen Bundestags, welche durch das *Dokumentations- und Informationssystem für Parlamentarische Vorgänge* öffentlich zugänglich sind [vgl. @dip21]. Diese Datenbank wird folglich als Datenquelle fungieren. Für sämtliche nachfolgend beschriebenen Schritte wurde die Programmiersprache R genutzt.[^1]

## Datenbeschaffung
Ausgehend der angeführten Punkte wurden sämtliche Drucksachen des 18. und 19. Bundestages automatisiert heruntergeladen.[^2] Da der 19. Bundestag nach wie vor besteht, musste ein Datum festgelegt werden, ab welchen keine weiteren Drucksachen mehr berücksichtigt werden. Der Zeitraum des 18. Bundestag beläuft sich vom 22. Oktober 2013 bis zum 24. Oktober 2017 und für den 19. Bundestag wurde der Zeitraum vom 24. Oktober bis zum 31. Dezember 2018 festgelegt. Für den 18. Bundestag wurden demnach 13.705 Drucksachen und für den 19. Bundestag 6.896 Drucksachen heruntergladen. Zusätzlich wurde die Trefferliste als CSV heruntergeladen, welche die Nummer der Drucksache sowie Titel und Datum der Drucksache enthält. Da jede Drucksache als PDF gespeichert wurde und ein standardisiertes Format aufweist, konnten die interessierten Bestandteile des Textes extrahiert werden.[^3] 

Zunächst wurde überprüft, ob es sich bei den Drucksachen um eine Anfrage handelt. Sofern es sich um keine Anfrage handelte, wurde die entsprechende Drucksache gelöscht. Anschließend wurden zudem die großen Anfragen ebenso gelöscht, da sich die Gesamtanzahl dieser über den gesamten Zeitraum auf unter 30 beläuft.[^4] Im 18. Bundestag wurden 3.951 kleine Anfragen gestellt und im 19. Bundestag 2.398. Der Datensatz beinhaltet somit 6349 kleine Anfragen. Anschließend wurde die Partei für jede Anfrage ermittelt, welche stets am Ende des Titels steht. Zudem wurde der Inhalt jeder Anfrage extrahiert, welcher für die spätere Kategorisierung herangezogen wird. Der hierdurch erstellte Datensatz wurde durch die heruntergeladen CSV-Datei mit Titel und Datum ergänzt.

## Klassifizierung

Im Hinblick auf die Forschungsfrage müssen die Anfragen nach Themen klassifiziert werden. Dabei müssen die Themen zuvor klar definiert werden, welche für die Klassifizierung herangezogen werden sollen. Hierbei soll sich verstärkt an der Arbeit von Baumgartner, Jones und Wilkerson orientiert werden, welche für die USA und der dortigen nationalen Politik ein Kodiersystem erstellt haben [vgl. @baumgartner2006comparative 970]. Neben dem Kodiersystem, welches für die US-Politik erstellt wurde, exisitert eine neuere Version, welche allgemeine Kodierrichtlinien beinhaltet [vgl. @cap]. Einerseits kann das Kodiersystem leicht auf verschiedene Länder angewendet werden. Andererseits ergibt sich durch die Verwendung dieses Kodiersystems die Möglichkeit der Vergleichbarkeit mit anderen Ländern, was jedoch in dieser Arbeit nicht weiter von Interesse ist [vgl. @john2006policy 983].

Anhand dieses Kodierungssystems werden die Anfragen in dieser Arbeit klassifiziert. Dabei enthält das Kodiersystem 21 Hauptthemen und 220 Subthemen. Zwischen den Subthemen wird bei der nachfolgenden Klassifizierung jedoch nicht differenziert. Diese Entscheidung ergibt sich aus zwei Gründen. Zum einen stellen die Hauptthemen bereits eine ausreichende Differenzierung zur Beantwortung der Forschungsfrage dar, wodurch eine zusätzliche Differenzierung zwischen den Subthemen keinen Mehrwert bringen würde. Zum anderen ist eine derart detaillierte Klassifizierung hinsichtlich der methodischen Vorgehensweise schwer zu realisieren, da es für einige der 220 Subthemen zu wenig Anfragen gibt. Eine Übersetzung der Kodierung ist im Anhang in Tabelle \ref{TabCodeBook} enthalten. Die Tabelle enthält die Hauptthemen sowie deren Subthemen. Ebenso sind in der Tabelle die Wörter enthalten, welche für den ersten Schritt der Klassifizierung herangezogen wurden.

Die Klassifizierung der Anfragen erfolgt in zwei Schritten. Im ersten Schritt werden diese anhand von Wörtern klassifiziert und anschließend erfolgt eine Validierung dieser Klassifizierung. Im zweiten Schritt erfolgt eine Klassifizierung der Anfragen, die nicht durch den ersten Schritt klassifiziert werden konnten. Hierfür werden zwei Verfahren des Supervised Machine Learnings verwendet. Auf diese beiden Schritte wird im Folgenden detailliert eingegangen.

### Wortklassifizierung

Anhand der Subthemen wurden für jedes Haupthema Wörter identifiziert, welche ausschließlich auf das jeweilige Haupthema zutreffen und sich thematisch von den übrigen Hauptthemen[^5] klar abgrenzen sollten. Dabei wurden die Wörter unter anderem auch durch das Querlesen einiger Anfragen ermittelt. Um mit Hilfe der Wörter eine zuverlässige Klassifizierung zu ermöglichen, wurde bei diesem Schritt lediglich der Titel der jeweiligen Anfrage herangezogen. Einerseits fungiert der Titel als eine Beschreibung des Inahlts, wodurch dieser entsprechend klar und prägnant formuliert sein muss. Andererseits ist davon auszugehen, dass im Inhalt der Anfrage ein Bezug zu anderen Themen vorkommen kann. Eine derartige Klassifizierung anhand der Inhalte würde folglich zu einem tendenziell unzuverlässigem Ergebnis führen.

Da im zweiten Schritt die bereits klassifizierten Anfragen als Grundlage dienen, wurde die Zuverlässigkeit der soeben beschriebene Klassifizierung überprüft. Dabei wurd zunächst analysiert, ob Anfragen mittels der verwendeten Klassifizierung zu mehreren Themen zugeordnet werden könnten. Anschließend müssen diese identifizierten Anfragen händisch klassifiziert werden. Zusätzlich wurden die Anfragen, welche nur einem Thema zugeordnet wurden, ebenso überprüft. Auf diese Weise soll eine hohe Zuverlässigkeit der Klassifizierung gewährleistet sein.

### Supervised Machine Learning

Durch die Klassifizierung mittels themenspezifischer Wörter können nicht alle Anfragen klassifiziert werden, wodurch ein weiterer Schritt notwendig sein wird. Ausgehend der klassifizierten Anfragen ergibt sich die Möglichkeit Methoden des Supervised Machine Learnings heranzuziehen. Dabei sollen zwei Klassifikationsverfahren verwendet werden. Es handelt sich um den Klassifikator *Random Forest* und um das Klassifikationsverfahren mittels *Support Vector Machines*. Die grundlegenden Konzepte beider Verfahren sollen dabei zunächst beschrieben werden. 

Ein Random Forest wird nach Breiman [-@breiman2001random] wie folgt definiert:

> Ein Random Forest ist ein Klassifikator, welcher aus einer Menge von *Tree*-strukturierten Klassifikatoren besteht {$h(\textbf{x}, \Theta_{k}), k = 1,...$}. Dabei handelt es sich bei {$\Theta_{k}$} um unabhängige zufällige Vektoren, welche eine identische Verteilung aufweisen. Jeder *Tree* gibt dabei eine Stimme für die beliebteste Klasse für \textbf{x} ab [vgl. @breiman2001random 6].

Der Algorithmus, welcher beim Random Forest[^6] zum Einsatz kommt, kann dabei wie folgt beschrieben werden [vgl. @liaw2002classification 18; vgl. @friedman2017elements 588]:

1. Aus dem Datensatz werden $n_{tree}$ Samples mittels Bootstrap gezogen.

2. Für jedes dieser Bootstrap-Samples wird ein *unprunted Tree* erstellt.[^7] Für die Erstellung der *Trees* wird der Gini Index $G$ verwendet, wobei $T$ das Training Set, $C_{i}$ die Klassen und $f(C_{i}, T)/|T|$ die Wahrscheinlichkeit, dass ein Fall zu der Klasse $C_{i}$ gehört, darstellt [vgl. @pal2005random 218; vgl. @chan2008evaluation 3002]: 
$$G = \sum \displaystyle\sum_{j \neq i}(f(C_{i}, T)/|T|)(f(C_{j}, T)/|T|)$$

3. Abschließend können mit einem solchen trainiertem Modell neue Daten mittels Aggregierung den Vorhersagen der einzelnen $n_{tree}$ *Trees* klassifiziert werden. Wenn $\hat{C}_{b}(x)$ die Klassenvorhersage des *b*ten Random Forest Tree ist, so ist für $\hat{C}_{rf}^{B}(x) = \text{Mehrheit der Stimmen}$ {$\hat{C}_{b}(x)$}$_{1}^{B}$ ($B = b_{1},..., b_{n}$).

Der Vorteil von Random Forest gegenüber einzelnen Decision Trees ist eine signifikant bessere Performance. Zudem ist es weniger anfällig hinsichtlich *Rauschen* [vgl. @ali2012random 274]. Jedoch kann bei (stark) ungleicher Verteilung der Klassen ein Bias in Richtung der *Majority Class* vorliegen [vgl. @boulesteix2012overview 18 f.].[^8]

Neben Random Forest gibt es u.a. die Support Vector Machines. Da diese eine etablierte und verbreitete Methode der Klassifikation darstellen und zudem als äußerst genau gelten [vgl. @ben2010user 223], soll in dieser Arbeit ebenso diese Methode herangezogen werden. Ursprünglich wurden Support Vector Machines für die binäre Klassifikation entwickelt [vgl. @cortes1995support 290]. Prinzipiell können Support Vector Machines nur binäre Klassifikationsprobleme lösen, jedoch gibt es Möglichkeiten mit welchen auch eine Multiklassifikation umgesetzt werden kann. 

Das grundlegende Konzept von Support Vector Machines soll anhand der Arbeit von @cortes1995support in Kürze dargelegt werden. Dabei definiert sich eine Support Vector Machine über drei Eigenschaften:

1. Es erfolgt eine Klassenseparierung durch das ermitteln einer *optimalen Hyperebene*. Bei einer binären Klassifikation und einem Input-Vektor von $x = (x_{1},...,x_{n})$ erfolgt die Zuweisung von $x$ zur positiven Klasse, wenn $f(x) \ge 0$ und zur negativen Klasse, wenn $f(x) < 0$. Dabei ist $f(x) = \displaystyle\sum_{i = 1}^{n}w_{i}x_{i}+b$, wobei $w$ der gewichtete Vektor und $b$ den Bias darstellt. Die Hyperebene definiert sich letztlich durch $\displaystyle\sum_{i = 1}^{n}w_{i}x_{i}+b = 0$ [vgl. @cristianini2000introduction 9 f.]. Die *optimale Hyperebene* separiert dabei die Daten mit der größtmöglichen bzw. der *optimalen Marge* zwischen den Klassen (siehe Abbildung \ref{svm_theory}). Fälle, welche auf den Grenzen der Marge liegen, stellen die *support vectors*  dar [vgl. @cortes1995support 275 ff.]. 

2. Ein aus Punkt 1 resultierendes Problem ist, dass das Training Set nicht immer ohne Fehlklassifizierungen separiert werden kann. Bei einer harten Marge könnte auf diese Weise keine Modellierung erfolgen. Um eine Separation mit möglichst wenig Fehlern von $\xi_{i}$ zu ermöglichen, wird bei der C-Support Vector Klassifikation[^9] die Konstante $C$ verwendet. Auf diese Weise wird eine *soft margin Hyperplane* erstellt, wobei mit größerem $C$ die Marge kleiner wird. [vgl. @cortes1995support 280 ff.; @chang2011libsvm: 3]:
\begin{equation*}
\begin{aligned}
& \underset{w, b, \xi}{\text{min}}
& & \displaystyle\frac{1}{2}w^2+CF \sum_{i = 1}^{l} \xi_{i}^{\sigma} \\
& \text{bezogen auf}
& & y_{i}(w*x_{i}+b) \ge 1- \xi_{i}, \quad i = 1,...,l_{i}, \\
&&& \xi_{i} \ge 0, \quad \quad \quad \quad \quad \quad \quad \quad i = 1,..., l.
\end{aligned}
\end{equation*}

3. Sofern eine lineare Seperation der Daten im *Input Space* nicht möglich ist, erfolgt eine Ermittlung der Hyperebene auf einem höher dimensionalen *Feature Space*. Auf diese Weise werden die Daten auf einen anderen Raum projeziert, wodurch eine lineare Separierung ermöglicht wird. Dies wird durch Kernel-Techniken[^10] realisiert [vgl. @cortes1995support 282 ff.; @cristianini2000introduction 26 ff.]. 

\begin{figure}[!h]
	\caption{Visuelle Darstellung einer linearen Support Vector Machine}
	\label{svm_theory}
	\centering
	\includegraphics[width = 0.77\textwidth]{images/svm_grafische_darstellung_theory.png}
	\caption*{\scriptsize Anmerkung: Darstellung anlehndend an Cortes \& Vapnik 1995 (S. 275)}
\end{figure}

Nachdem das grundlegende Konzept von Support Vector Machines aufgezeigt wurde, wird deutlich weshalb prinzipiell nur eine binäre Klassifikation möglich ist. Für eine Multiklassifikation wurden jedoch Methoden entwickelt, wodurch Support Vector Machines ebenso für solche Klassifikationsprobleme verwendet werden können. In dieser Arbeit soll die Methode *one-against-one* genutzt werden. Hierbei werden für $k$-Klassen $k(k-1)/2$ Klassifikatoren bzw. Support Vector Machines erstellt. Ebenso wie beim Random Forest erfolgt die finale Zuweisung der Klasse mittels Voting, wobei $x$ zu der Klasse zugewiesen wird, für welche die größte Anzahl an Stimmen vorliegt [vgl. @chang2011libsvm 29 f.]. Darüber hinaus ist es möglich ebenso die Klassenwahrscheinlichkeit zu berechnen [siehe @wu2004probability].

Die soeben beschriebenen Algorithmen können in R mit dem Package `RTextTools` [@RTextTools; @collingwood2013rtexttools] angewendet werden. Hierbei handelt es sich um ein Package, welches explizit für die Klassifikation von Texten entwickelt wurde und verschiedene Packages vereint. So ist für Random Forest das Package `randomForest` [@randomForest] und für Support Vector Machines das Package `e1071` [@e1071] implementiert.[^11] Für die Klassifikation werden die bereits klassifizierten Anfragen als Input herangezogen. Zugleich werden die Modelle mittels der Inhalte der Anfragen trainiert. Dabei müssen die Texte der Inhalte zuvor von den Stoppwörtern, den Punktuationen und den Wörtern, welche nur bei äußerst wenigen Dokumenten vorkommen, bereinigt werden, da dies für den Inhalt und der Klassifikation nicht relevant ist. Zugleich müssen die Texte der Inhalte in ein strukturiertes Format umgewandelt werden, was durch die Erstellung einer Document-Term-Matrix erfolgt [vgl. @meyer2008text 5 \& 23]. Vor der richtigen Klassifikation soll anhand der bereits klassifizierten Anfragen die Perfromance beider Klassifikationsverfahren verglichen werden.

Im Anschluss an die Klassifizierung kann untersucht werden in welcher Weise sich die Parteien hinsichtlich der thematischen Schwerpunkte voneinander unterscheiden. Ebenso können die Unterschiede zwischen dem 18. und 19. Bundestag herausgearbeitet werden.

### Sentiment Analyse

Da davon auszugehen ist, dass die Parteien hinsichtlich der Themen unterschiedliche Sentiments aufweisen können, soll ebenso eine Sentiment Analyse durchgeführt werden. Mittels eines Wörterbuchs soll das durchschnittliche *Sentiment* der einzelnen Anfragen ermittelt werden. Hierbei wird das öffentlich zugängliche deutsche Wörterbuch *SentiWS* verwendet [@sentiws]. Dieses enthält Wörter mit positiven und negativen (numerischen) Konnotationen. Eine detaillierte Beschreibung dieses Wörterbuchs sowie der verwendeten Methoden zwecks Erstellung ist in @remus2010sentiws und @goldhahn12building vorzufinden. Für die Sentiment Analyse wird das Package `SentimentAnalysis` genutzt [@sentimentanalysis]. Hiermit wird für jede Anfrage ein Sentiment ermittelt: [^12]
$$Sentiment = \displaystyle\frac{\displaystyle\sum_{i=1}^{n}{p_{i} - \displaystyle\sum_{i=1}^{n}{n_{i}}}}{\displaystyle\sum_{i = 1}^{n}{w_{i}}}$$

Hierbei wird die Differenz zwischen der Summe positiver, $p$, und negativer Wörter, $n$, durch die Gesamtanzahl der Wörter, $w$, dividiert. Auf diese Weise wird ebenso die Größe des Dokuments berücksichtigt. Aufgrund der hohen Anzahl der Themen soll die Analyse dabei auf die am häufigsten aufkommenden Themen beschränkt werden.

[^1]: Der Code für alle Auswertungen der Hausarbeit ist auf GitHub unter https://github.com/MCStatistic/SeminarPaper_ASGB einsehbar.
[^2]: Hierfür wurde eine Funktion in R geschrieben, mit welcher jede Drucksache als PDF vom offiziellen Server heruntergeladen wurde. Die Dateien wurden dabei unter der Nummer der Drucksache sowie des jeweiligen Bundestags gespeichert um so eine klare Zuordnung gewährleisten zu können.
[^3]: Hierfür wurde das Package *pdftools* [vgl. @pdftools] genutzt.
[^4]: Da aus diesem Grund die großen Anfragen in dieser Arbeit nicht berücksichtigt werden, wird nachfolgend zwecks der Lesbarkeit von *Anfragen* geschrieben. Dieser Begriff wird in dieser Arbeit äquivalent zu *kleinen Anfragen* genutzt.
[^5]: Da keine Differenzierung zwischen den Subthemen erfolgt und nachfolgend nur noch eine Differenzierung zwischen den Hauptthemen relevant ist, wird ab sofort nur noch von Themen geschrieben. Die Begriffe *Hauptthema* und *Thema* werden in dieser Arbeit daher äquivalent genutzt.
[^6]: Grundlegend handelt es sich um eine Ansammlung von Descision Trees, welche hier jedoch nicht im Detail ausgeführt werden sollen. Dabei sei auf die Arbeit von @breiman1984tree und @banfield2007comparison verwiesen.
[^7]: Um ein Overfitting bei Decision Trees zu vermeiden, wird die Methodik des *Prunings* verwendet [siehe @bradford1998pruning; @mingers1989empirical; @mehta1995mdl]. Dies ist beim Random Forest jedoch nicht erforderlich, da durch das Bootstrapping (oder auch *Bagging*) der Fehler bereits signifikant reduziert wird [siehe hierzu @breiman1996bagging; @prasad2006newer 184]. Die Gefahr eines Overfittings ist bei einem Random Forest daher kein Problem [vgl. @breiman2001random].
[^8]: Zwar gibt es Möglichkeiten um diesem Problem entgegenzuwirken, jedoch soll stattdessen eine Kombination der beiden herangezogenen Klassifikationsverfahren verwendet werden, sofern dies erforderlich sein sollte.
[^9]: C-Support Vector Klassifikatoren werden auch in dieser Arbeit herangezogen. Aus diesem Grund werden andere Lösungen für das angeführte Problem nicht thematisiert.
[^10]: Hierbei soll vorerst nicht näher eingegangen werden, da es viele verschiedene Kernel-Techniken gibt. Für eine allgemeine detaillierte Übersicht diverser Kernel-Techniken bietet sich die Arbeit von @cristianini2000introduction an. Für diese Arbeit wird ein lineares Kernel verwendet.
[^11]: Die detaillierten Algorithmen, welche bei den Packages zum Einsatz kommen, können für randomForest in @breiman2001random und für Support Vector Machines in @chang2011libsvm vorgefunden werden. 
[^12]: Die hier aufgeführte Berechnung entspricht der Logik, welche im Package `SentimentAnalysis` unter der Funktion `ruleSentiment()` vorzufinden ist. Dies kann im Reference Manual des Packages eingesehen werden [@sentimentanalysis].
